# ==============================================================================
# INSTALLATIONS (Run once per new Colab runtime)
# ==============================================================================
# !pip install -U sentence-transformers transformers spacy tensorflow keras --progress-bar off
# !python -m spacy download it_core_news_sm

# ==============================================================================
# MAIN SCRIPT
# ==============================================================================
import os
import gc
import re
import shutil
import time

import tensorflow as tf
import keras
# import keras_tuner as kt # REMOVED Keras Tuner
import numpy as np
import pandas as pd
import spacy
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split # Removed StratifiedKFold for now
from sklearn.preprocessing import LabelEncoder, StandardScaler
from google.colab import drive

def initial_gpu_check():
    print(f"TensorFlow Version: {tf.__version__}")
    print(f"Keras Version: {keras.__version__}")

    gpus = tf.config.list_physical_devices('GPU')
    print(f"Physical GPUs available: {gpus}")

    if gpus:
        try:
            print("Attempting to set memory growth for GPUs...")
            for gpu_device in gpus:
                tf.config.experimental.set_memory_growth(gpu_device, True)
            print("GPU memory growth set successfully.")

            print("Performing a simple TensorFlow operation on GPU...")
            with tf.device('/GPU:0'): # Explicitly use the first GPU
                a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
                b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
                c = tf.matmul(a, b)
            print(f"Simple TF GPU matmul test successful. Result matrix:\n{c.numpy()}")
            print("Successfully initialized and tested GPU.")
            return True
        except RuntimeError as e:
            print(f"!!!!!!!! ERROR DURING GPU INITIALIZATION OR TEST: {e} !!!!!!!!")
            print(f"Colab environment issue. Disconnect/delete runtime, ensure GPU, rerun installs & script.")
            return False
    else:
        print(f"!!!!!!!! NO GPU DETECTED BY TENSORFLOW !!!!!!!!")
        print("Ensure 'GPU' is selected in 'Runtime' -> 'Change runtime type'.")
        return False

if not initial_gpu_check():
    raise SystemExit("GPU initialization failed. Stopping script.")

drive.mount('/content/drive', force_remount=True)

BASE_DIR = '/content/drive/MyDrive/tayyab'
EMB_DIR = os.path.join(BASE_DIR, 'embeddings_fe') # Versioning - Keeping this as features/embeddings themselves are not changing
RESULT_CSV = os.path.join(BASE_DIR, 'deep_classifier_results_manual_dl_v3.csv') # Versioning - New results file

os.makedirs(EMB_DIR, exist_ok=True)

if os.path.exists(RESULT_CSV):
    print(f"Removing existing results file: {RESULT_CSV}")
    os.remove(RESULT_CSV)


print("Loading and cleaning data...")
csv_path = os.path.join(BASE_DIR, 'italian_crime_news.csv')
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"CSV file not found at: {csv_path}")
df = pd.read_csv(csv_path)
df.dropna(subset=['text'], inplace=True)
df['text'] = df['text'].astype(str)
texts_full = df['text'].tolist()
labels_full = df['word2vec_tag'].tolist()

del df; gc.collect()

print("Encoding labels...")
en = LabelEncoder()
y_encoded_full = en.fit_transform(labels_full)
num_classes = len(en.classes_)

X_train_texts, X_test_texts, y_train_np, y_test_np = train_test_split(
    texts_full, y_encoded_full, test_size=0.2, stratify=y_encoded_full, random_state=42
)
del texts_full, labels_full, y_encoded_full; gc.collect()


print("Setting up spaCy for feature extraction...")
components_to_exclude = ["parser", "tagger", "attribute_ruler", "lemmatizer"]
try:
    nlp = spacy.load('it_core_news_sm', exclude=components_to_exclude)
    print(f"spaCy pipeline loaded with components: {nlp.pipe_names}")
    if not (any(comp in nlp.pipe_names for comp in ['senter', 'sentencizer'])):
        print("Warning: No sentence boundary detector found. Adding 'sentencizer'.")
        nlp.add_pipe("sentencizer", first=True)
    if 'ner' not in nlp.pipe_names:
        print("Warning: NER component not found, but doc.ents is used. Consider adjusting excludes.")

except OSError:
    print("Downloading 'it_core_news_sm' spaCy model...")
    # This line is for Colab/Jupyter environments.
    # If running locally and spacy is not in a virtual env, you might need to adjust.
    import subprocess
    subprocess.run(['python', '-m', 'spacy', 'download', 'it_core_news_sm'])
    nlp = spacy.load('it_core_news_sm', exclude=components_to_exclude)
    print(f"spaCy pipeline loaded after download with components: {nlp.pipe_names}")
    if not (any(comp in nlp.pipe_names for comp in ['senter', 'sentencizer'])):
        print("Warning: No sentence boundary detector found. Adding 'sentencizer'.")
        nlp.add_pipe("sentencizer", first=True)


crime_kw = {'omicidio','furto','arresto','rapina','droga','corruzione'}
kw_pattern = re.compile(r"\b(?:" + "|".join(map(re.escape, crime_kw)) + r")\b", flags=re.IGNORECASE)

def extract_feats(input_texts):
    feats = []
    for doc in nlp.pipe(input_texts, batch_size=128):
        t = doc.text
        num_chars = len(t); num_words = len(doc)
        alpha = [tok for tok in doc if tok.is_alpha]
        avg_w = np.mean([len(tok.text) for tok in alpha]) if alpha else 0
        sents = sum(1 for _ in doc.sents)
        up = sum(1 for tok in doc if tok.is_upper and tok.is_alpha)
        stp = sum(1 for tok in doc if tok.is_stop)
        ents = len(doc.ents)
        has_kw = int(bool(kw_pattern.search(t)))
        feats.append([num_chars, num_words, avg_w, sents, up, stp, ents, has_kw])
    return np.array(feats, dtype=np.float32)

train_feats_fpath = os.path.join(EMB_DIR, 'train_feats.npy')
if os.path.exists(train_feats_fpath):
    print("Loading TRAIN engineered features from cache...")
    X_train_feats = np.load(train_feats_fpath)
else:
    print("Extracting TRAIN engineered features ..."); X_train_feats = extract_feats(X_train_texts)
    print("Saving TRAIN engineered features ..."); np.save(train_feats_fpath, X_train_feats)

test_feats_fpath = os.path.join(EMB_DIR, 'test_feats.npy')
if os.path.exists(test_feats_fpath):
    print("Loading TEST engineered features from cache...")
    X_test_feats = np.load(test_feats_fpath)
else:
    print("Extracting TEST engineered features ..."); X_test_feats = extract_feats(X_test_texts)
    print("Saving TEST engineered features ..."); np.save(test_feats_fpath, X_test_feats)


scaler_feats = StandardScaler()
X_train_feats = scaler_feats.fit_transform(X_train_feats)
X_test_feats  = scaler_feats.transform(X_test_feats)

print("Setting up SentenceTransformer model...")
try:
    sent_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
except Exception as e: print(f"Error loading SentenceTransformer model: {e}"); raise

def get_emb(name, input_texts_list, model_emb_dir):
    cache = os.path.join(model_emb_dir, f'{name}_emb_manual.npy')
    if os.path.exists(cache):
        print(f"Loading {name} embeddings from cache: {cache}"); return np.load(cache)

    print(f"Generating {name} embeddings...")
    emb_device = 'cuda' if tf.config.list_physical_devices('GPU') else 'cpu'
    print(f"Using device: {emb_device} for sentence embeddings.")

    embeddings = sent_model.encode(
        input_texts_list, batch_size=32, show_progress_bar=True,
        device=emb_device, normalize_embeddings=True
    )
    print(f"Saving {name} embeddings to cache: {cache}"); np.save(cache, embeddings)
    return embeddings

emb_train = get_emb('train', X_train_texts, EMB_DIR)
emb_test  = get_emb('test',  X_test_texts, EMB_DIR)


print("Releasing SentenceTransformer model and text lists from memory...")
del sent_model; del X_train_texts; del X_test_texts; gc.collect()

print("Combining features and embeddings...")
X_train_final = np.hstack([emb_train, X_train_feats]); del emb_train, X_train_feats; gc.collect()
X_test_final  = np.hstack([emb_test,  X_test_feats]);  del emb_test, X_test_feats;  gc.collect()

input_dim = X_train_final.shape[1]

print(f"Final X_train_final shape: {X_train_final.shape}, y_train_np shape: {y_train_np.shape}")
print(f"Input dimension for model: {input_dim}, Number of classes: {num_classes}")


def build_and_compile_model(params_dict, current_input_dim, current_num_classes):
    keras.backend.clear_session(); gc.collect()

    model = keras.Sequential()
    model.add(keras.layers.Input(shape=(current_input_dim,)))

    # First hidden layer (always present if num_hidden_layers >= 1)
    model.add(keras.layers.Dense(units=params_dict.get('units_0', 32), activation='relu'))
    model.add(keras.layers.Dropout(rate=params_dict.get('dropout_0', 0.2)))

    if params_dict.get('num_hidden_layers', 1) >= 2: # Second hidden layer
        model.add(keras.layers.Dense(units=params_dict.get('units_1', 32), activation='relu'))
        model.add(keras.layers.Dropout(rate=params_dict.get('dropout_1', 0.2)))

    if params_dict.get('num_hidden_layers', 1) >= 3: # Third hidden layer
        model.add(keras.layers.Dense(units=params_dict.get('units_2', 16), activation='relu')) # Default units for 3rd layer
        model.add(keras.layers.Dropout(rate=params_dict.get('dropout_2', 0.1))) # Default dropout for 3rd layer

    model.add(keras.layers.Dense(current_num_classes, activation='softmax'))

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=params_dict.get('learning_rate', 1e-4)),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'],
    )
    return model

# Updated model configurations: more complex models added
model_configurations = [
    # Original configurations (can be kept for baseline or commented out)
    {'name': 'model_1_small_1layer', 'units_0': 32, 'dropout_0': 0.2, 'num_hidden_layers': 1, 'learning_rate': 1e-4},
    {'name': 'model_2_medium_1layer', 'units_0': 64, 'dropout_0': 0.3, 'num_hidden_layers': 1, 'learning_rate': 1e-4},
    {'name': 'model_3_small_2layer', 'units_0': 32, 'dropout_0': 0.2, 'units_1': 32, 'dropout_1': 0.2, 'num_hidden_layers': 2, 'learning_rate': 1e-4},
    {'name': 'model_4_medium_2layer_high_lr', 'units_0': 64, 'dropout_0': 0.3, 'units_1': 32, 'dropout_1': 0.2, 'num_hidden_layers': 2, 'learning_rate': 1e-3},

    # New, more complex configurations
    {'name': 'model_5_large_1layer', 'units_0': 128, 'dropout_0': 0.3, 'num_hidden_layers': 1, 'learning_rate': 1e-4},
    {'name': 'model_6_medium_2layer_v2', 'units_0': 128, 'dropout_0': 0.3, 'units_1': 64, 'dropout_1': 0.2, 'num_hidden_layers': 2, 'learning_rate': 1e-4},
    {'name': 'model_7_small_3layer_taper', 'units_0': 64, 'dropout_0': 0.2, 'units_1': 32, 'dropout_1': 0.2, 'units_2': 16, 'dropout_2': 0.1, 'num_hidden_layers': 3, 'learning_rate': 1e-4},
    {'name': 'model_8_medium_3layer_taper', 'units_0': 128, 'dropout_0': 0.3, 'units_1': 64, 'dropout_1': 0.2, 'units_2': 32, 'dropout_2': 0.1, 'num_hidden_layers': 3, 'learning_rate': 1e-4},
    {'name': 'model_9_large_3layer_taper_high_lr', 'units_0': 256, 'dropout_0': 0.4, 'units_1': 128, 'dropout_1': 0.3, 'units_2': 64, 'dropout_2': 0.2, 'num_hidden_layers': 3, 'learning_rate': 5e-4}, # Even more complex
]

all_results = []
EPOCHS_PER_MODEL = 15 # Increased epochs
BATCH_SIZE_TRAIN = 32

for i, config in enumerate(model_configurations):
    print(f"\n--- Training Configuration: {config['name']} ({i+1}/{len(model_configurations)}) ---")

    print(f"Building model: {config['name']}")
    model = build_and_compile_model(config, input_dim, num_classes)
    model.summary()

    early_stop_cb = keras.callbacks.EarlyStopping(
        monitor='val_loss', patience=3, restore_best_weights=True, verbose=1 # Increased patience slightly
    )

    history = None; training_successful = False
    try:
        print(f"Starting training for {config['name']}...")
        time.sleep(1)

        history = model.fit(
            X_train_final, y_train_np,
            epochs=EPOCHS_PER_MODEL,
            batch_size=BATCH_SIZE_TRAIN,
            validation_split=0.15,
            callbacks=[early_stop_cb],
            verbose=1
        )
        print(f"Training completed for {config['name']}.")
        training_successful = True

    except tf.errors.FailedPreconditionError as e_tf:
        print(f"!!!!!!!! TENSORFLOW FAILED PRECONDITION ERROR for {config['name']}: {e_tf} !!!!!!!!")
    except Exception as e_generic:
        print(f"!!!!!!!! GENERIC ERROR during training of {config['name']}: {e_generic} !!!!!!!!")
        import traceback; traceback.print_exc()

    if training_successful and history:
        print(f"Evaluating {config['name']} on test set...")
        test_loss, test_acc = model.evaluate(X_test_final, y_test_np, batch_size=BATCH_SIZE_TRAIN, verbose=0)
        print(f"Test Results for {config['name']} - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}")

        # Ensure history has the keys before accessing
        final_val_loss = history.history['val_loss'][-1] if history.history and 'val_loss' in history.history and history.history['val_loss'] else None
        final_val_accuracy = history.history['val_accuracy'][-1] if history.history and 'val_accuracy' in history.history and history.history['val_accuracy'] else None

        # If early stopping restored best weights, we might want the best val_loss/acc, not the last.
        # However, for simplicity and consistency with previous script, keeping last.
        # To get best: best_epoch_idx = np.argmin(history.history['val_loss'])
        # final_val_loss = history.history['val_loss'][best_epoch_idx]
        # final_val_accuracy = history.history['val_accuracy'][best_epoch_idx]


        current_result = {
            'model_name': config['name'], **config,
            'test_loss': test_loss, 'test_accuracy': test_acc,
            'final_val_loss': final_val_loss,
            'final_val_accuracy': final_val_accuracy,
            'epochs_run': len(history.history['loss']) # Actual epochs run if early stopped
        }
        all_results.append(current_result)
    else:
        print(f"Skipping evaluation for {config['name']} due to training failure or no history.")
        current_result = {
            'model_name': config['name'], **config,
            'test_loss': None, 'test_accuracy': None,
            'final_val_loss': None, 'final_val_accuracy': None,
            'epochs_run': 0
        }
        all_results.append(current_result) # Still log the config even if it failed

    del model; del history; gc.collect(); keras.backend.clear_session()
    print(f"Cleaned up after {config['name']}. Waiting a bit...")
    time.sleep(2)

if all_results:
    df_all_results = pd.DataFrame(all_results)
    print("\n--- All Model Results ---"); print(df_all_results)
    df_all_results.to_csv(RESULT_CSV, index=False)
    print(f"All results saved to {RESULT_CSV}")
else:
    print("No models were processed or produced results.")

print("Script finished.")
